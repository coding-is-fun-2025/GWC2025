{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89925d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1 Load embeddings model\n",
    "from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "\n",
    "embedding_model = LlamaCppEmbeddings(model_path=\"models/all-MiniLM-L6-v2-Q6_K.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.2 load vector database that was persisted in the past \n",
    "\n",
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_folder = 'chroma_db_c1000o200_docs_textbooks'\n",
    "client = chromadb.PersistentClient(path=persist_folder) \n",
    "vectordb = Chroma(\n",
    "    client=client,\n",
    "    embedding_function=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.3 load a LLM model\n",
    "\n",
    "# download LLM models from https://huggingface.co/\n",
    "#   note: search for gguf models\n",
    "#         and download .gguf file\n",
    "#         and make sure the model_path is pointed to it\n",
    "\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "#models/gemma-3-12b-it-q4_0.gguf\n",
    "#models/Meta-Llama-3.1-8B-Instruct-Q6_K_L.gguf\n",
    "#models/google_gemma-3n-E4B-it-Q6_K.gguf\n",
    "#models/llama-2-7b.Q4_K_M.gguf\n",
    "\n",
    "## https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"models/gemma-3-12b-it-q4_0.gguf\",   # <--- make sure this is pointed to your folder/model\n",
    "    n_gpu_layers=100,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# Step 3.4 connect the LLM and the vector database for question and answer\n",
    "########################################################################################\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3778557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.5 ask a question\n",
    "\n",
    "query = \"\"\"\n",
    "Which type of bond represents a weak chemical bond?\n",
    "a. hydrogen bond\n",
    "b. ionic bond\n",
    "c. covalent bond\n",
    "d. polar covalent bond\n",
    "\"\"\"\n",
    "response = qa.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10786cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3.6 (extra) display relevant text retrieved from vectordb\n",
    "\n",
    "print(f'using chroma db at {persist_folder} to search for relevant text')\n",
    "results = vectordb.similarity_search_with_score(query)\n",
    "# Returns List of documents most similar to the query text and cosine distance in float for each. Lower score represents more similarity.\n",
    "# https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.chroma.Chroma.html#langchain_community.vectorstores.chroma.Chroma.similarity_search_with_relevance_scores\n",
    "\n",
    "\n",
    "# results is a list []\n",
    "#   but its contents is a tuple ()\n",
    "#     the first item in the tuple being the document which contains metadata and page_content\n",
    "#     the second item in the tuple is the cosine distince\n",
    "\n",
    "# Cosine Distance:\n",
    "# Usually, people use the cosine similarity as a similarity metric between vectors. the cosine distance can be defined as follows:\n",
    "# Cosine Distance = 1 â€” Cosine Similarity\n",
    "# The intuition behind this is that if 2 vectors are perfectly the same then the similarity is 1 (angle=0 hence ð‘ð‘œð‘ (ðœƒ)=1) and thus, distance is 0 (1â€“1=0).\n",
    "# https://medium.com/geekculture/cosine-similarity-and-cosine-distance-48eed889a5c4\n",
    "\n",
    "for i in results:\n",
    "    print(f'cosine distance {i[1]}\\n', i[0].page_content,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0683d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the end!\n",
    "\n",
    "# how does a neural network work?\n",
    "# https://youtu.be/AeM5LgNmNAw?feature=shared&t=141\n",
    "# https://youtu.be/me4SV_tuMSE?feature=shared&t=312\n",
    "# https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\n",
    "\n",
    "# how does an LLM transformer architecture work?\n",
    "# https://youtu.be/wjZofJX0v4M?feature=shared\n",
    "# https://youtu.be/bCz4OMemCcA?feature=shared\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
